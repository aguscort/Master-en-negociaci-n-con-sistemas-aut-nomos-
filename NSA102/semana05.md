### Semana 5: Desafíos en MARL

1.  **La No-Estacionariedad (Non-Stationarity)**
    *   **Concepto:** En el Aprendizaje por Refuerzo para un solo agente (como en un MDP), el entorno se considera **estacionario**. Esto significa que las probabilidades de transición de estado $p(s'|s,a)$ y la función de recompensa $r(s,a)$ son fijas e independientes del tiempo. El agente aprende una política óptima con respecto a esta dinámica fija.
    *   **El Desafío en MARL:** En un sistema multiagente, desde la perspectiva de un **agente individual**, el "entorno" incluye a los *otros agentes*. Cuando cada agente está aprendiendo y actualizando su propia política simultáneamente, la política **conjunta** de los demás agentes (y por lo tanto, la dinámica efectiva del entorno percibida por un agente) está en constante cambio. Esto hace que el entorno sea **no-estacionario** desde la perspectiva de un solo agente.
    *   **Consecuencia:** Los algoritmos de RL diseñados para entornos estacionarios pueden fallar en converger en entornos MARL no-estacionarios, o pueden converger a políticas subóptimas o cíclicas (donde las políticas de los agentes se persiguen mutuamente sin estabilizarse).
    *   **Fuente de los Materiales:** Las fuentes proporcionadas discuten la formalización de juegos con múltiples jugadores y la toma de decisiones y el aprendizaje por refuerzo en MDPs (que son estacionarios). Sin embargo, los extractos **no discuten explícitamente el desafío de la no-estacionariedad** de un entorno multiagente desde la perspectiva de un agente individual aprendiendo simultáneamente con otros. Este es un concepto fundamental en MARL, pero no se detalla en los materiales proporcionados.

2.  **El Problema del "Credit Assignment" Multiagente**
    *   **Concepto (RL un solo agente):** En RL para un solo agente, el problema del "credit assignment" (asignación de crédito) es determinar **cuáles acciones pasadas** fueron responsables de recibir una recompensa (especialmente cuando las recompensas son escasas y se reciben mucho después de las acciones que las causaron).
    *   **El Desafío en MARL:** En MARL, este problema se vuelve más complejo porque hay que considerar no solo el aspecto **temporal** (qué acciones pasadas *de este agente* importan), sino también el aspecto **inter-agente** (qué acciones *de otros agentes*, o la combinación de acciones de todos, contribuyeron al resultado o recompensa).
        *   En entornos **cooperativos** con recompensa compartida, si el equipo recibe una recompensa grande, ¿cómo sabe un agente individual si fue su acción la que más contribuyó o si fue la acción de otro agente?. Un agente necesita aprender a coordinarse, pero sin una forma clara de atribuir el éxito o el fracaso a acciones individuales, esto es difícil.
        *   En entornos **competitivos** o **mixtos**, un agente necesita entender cómo las acciones de sus oponentes afectan su propia recompensa y las transiciones de estado, además de cómo sus propias acciones contribuyen a su resultado.
    *   **Fuente de los Materiales:** Los materiales mencionan la idea de "accurately assigning credit for long-term consequences to individual action selections" en el contexto de MDPs de un solo agente. Sin embargo, los extractos **no profundizan en cómo este problema se agrava en el contexto multiagente**, donde la asignación de crédito debe considerar la contribución de múltiples agentes a un resultado conjunto o la influencia de las acciones de otros agentes en la recompensa o el estado de un agente individual.

3.  **Escalabilidad**
    *   **Concepto:** La escalabilidad se refiere a la capacidad de un algoritmo o sistema para funcionar eficazmente a medida que el tamaño del problema aumenta (en este caso, el número de agentes, la complejidad del estado o el espacio de acción).
    *   **El Desafío en MARL:** El principal problema de escalabilidad en MARL surge del tamaño del **espacio de acción conjunta** y, a veces, del **espacio de estado conjunta**.
        *   Si hay $N$ agentes y cada agente tiene un conjunto de acciones de tamaño $|A_i|$, el número total de **acciones conjuntas** posibles es el producto de los tamaños de los conjuntos de acciones individuales: $|A_1| \times |A_2| \times \dots \times |A_N|$. Este número **crece exponencialmente con el número de agentes**.
        *   En un marco de Juego Estocástico, las transiciones de estado y las recompensas dependen de esta acción conjunta. Un agente que intente modelar explícitamente a todos los demás agentes o aprender el valor de cada acción conjunta se enfrentará rápidamente a un problema intratable debido a esta explosión combinatoria.
        *   Incluso en un entorno cooperativo con un planificador central, el gran número de acciones globales (acciones conjuntas) hace que resolver el problema sea "difícil" si requiere enumerar todas las combinaciones posibles.
        *   Los sistemas complejos con muchos agentes y interacciones también presentan desafíos de escalabilidad en términos de manejo y predicción de comportamiento. Un ejemplo de tarea compleja mencionada es el movimiento de una varilla con obstáculos, que se considera "probablemente demasiado grande para ser resuelto con métodos sin prioridad".
    *   **Fuente de los Materiales:** Las fuentes **sí identifican directamente** la escalabilidad como un desafío importante. Mencionan explícitamente que el número de acciones globales es exponencial en el número de agentes. Reconocen que esto hace que resolver el MDP (incluso con un planificador central) sea "duro" si se basa en la enumeración de acciones conjuntas. También señalan que algunas representaciones de juegos implícitamente especifican espacios de acción que son exponenciales y que se requieren métodos más avanzados (como el barrido priorizado) para tareas demasiado grandes.

En resumen, aunque los materiales proporcionados se centran más en la formalización de los juegos y los mecanismos de diseño, sí destacan la **escalabilidad** como un desafío clave debido a la naturaleza combinatoria de las acciones conjuntas en sistemas multiagente. Los otros desafíos importantes en MARL, la **no-estacionariedad** (causada por el aprendizaje simultáneo de otros agentes) y el **problema multiagente de asignación de crédito** (cómo atribuir el éxito/fracaso entre agentes), son conceptos fundamentales en el campo de MARL que complementan la comprensión de los Juegos Estocásticos, pero que **no son detallados explícitamente** en los extractos de las fuentes proporcionadas.
